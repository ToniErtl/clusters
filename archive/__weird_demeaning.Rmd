---
title: "Elrontott Demeaned clustering"
author: "Antal Ertl"
date: '2023 03 01 '
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE) # not include the code


library(tidyverse)
#library(tidymodels)
library(ggplot2)
library(ggthemes)

library(rgl) # 3 dimensional plots
library(factoextra)
library(crosstable) # crosstable function


# Clustering methods:
library(cluster)

# For random forest modeling and visualization:
library(rpart)
library(partykit)
library(party)

# Additional packages for html output:

library(knitr)
library(kableExtra)


corrplot2 <- function(data,
                      method = "pearson",
                      sig.level = 0.05,
                      order = "original",
                      diag = FALSE,
                      type = "upper",
                      tl.srt = 90,
                      number.font = 1,
                      number.cex = 1,
                      mar = c(0, 0, 0, 0)) {
  library(corrplot)
  data_incomplete <- data
  data <- data[complete.cases(data), ]
  mat <- cor(data, method = method)
  cor.mtest <- function(mat, method) {
    mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat <- matrix(NA, n, n)
    diag(p.mat) <- 0
    for (i in 1:(n - 1)) {
      for (j in (i + 1):n) {
        tmp <- cor.test(mat[, i], mat[, j], method = method)
        p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
      }
    }
    colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
    p.mat
  }
  p.mat <- cor.mtest(data, method = method)
  col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
  corrplot(mat,
    method = "color", col = col(200), number.font = number.font,
    mar = mar, number.cex = number.cex,
    type = type, order = order,
    addCoef.col = "black", # add correlation coefficient
    tl.col = "black", tl.srt = tl.srt, # rotation of text labels
    # combine with significance level
    p.mat = p.mat, sig.level = sig.level, insig = "blank",
    # hide correlation coefficiens on the diagonal
    diag = diag
  )
}

```

```{r load data}
data <- read.csv(".//Data in brief//Horn-Kiss-Lenard2021.csv")
```

<!-- demeanelni osztályonként az összes score-t: -->

<!-- - classid-nként megnézni az átlagot és ahhoz képest mennyi standard eloszlásra van az adott ember adott scoreból -->

<!-- ezekre a deamenelt scoreokra futtatni az egészet -->

<!-- osztály átlag hisztogramok az egyes nem demeanelt változókra. -->

<!-- Április 20-21 -->

---------------------------------------------


```{r}

pca_data <- data %>%  mutate(STournamentChoosed =replace_na(STournamentChoosed,0)) %>%
        mutate(STournamentChoosed_comp =STournamentChoosed* comp) %>%
                     dplyr::select(studid,
                                   classid,
                                   delta,
                                   #delta_now,
                                   beta,
                                   #pb,
                                   beta_0, # time inconsistency
                                   dictator, #altruism
                                   dictator_schoolmate, #altruism
                                   risk, # risk
                                   publicgood, # cooperativeness
                                   trust, # trust
                                   trust_return) %>%  #trust
  drop_na() 



# demean: redefine variable where we indicate for each individual how many standard deviations he or she is from the class average

pca_data %>%  mutate(STournamentChoosed =replace_na(STournamentChoosed,0)) %>%
                    mutate(STournamentChoosed_comp =STournamentChoosed* comp) %>%
                     dplyr::select(studid,
                                   classid,
                                   delta,
                                   #delta_now,
                                   beta,
                                   #pb,
                                   beta_0, # time inconsistency
                                   dictator, #altruism
                                   dictator_schoolmate, #altruism
                                   risk, # risk
                                   publicgood, # cooperativeness
                                   trust, # trust
                                   trust_return) %>%  #trust
  drop_na() %>% 
  group_by(classid) %>% 
  mutate(#mean values: 
         mean_delta= mean(delta),
         mean_beta = mean(beta),
         mean_beta_0 = mean(beta_0),
         mean_dictator = mean(dictator),
         mean_dictator_schoolmate = mean(dictator_schoolmate),
         mean_risk = mean(risk),
         mean_publicgood = mean(publicgood),
         mean_trust = mean(trust),
         mean_trust_return = mean(trust_return),
         #standard deviations:
         sd_delta = sd(delta),
         sd_beta = sd(beta),
         sd_beta_0 = sd(beta_0),
         sd_dictator = sd(dictator),
         sd_dictator_schoolmate = sd(dictator_schoolmate),
         sd_risk = sd(risk),
         sd_publicgood = sd(publicgood),
         sd_trust = sd(trust),
         sd_trust_return = sd(trust_return)) %>% 
  ungroup() %>% 
  # calculate how many standard deviations one observation is from the mean
  # mutate(delta_diff = (delta - mean_delta ) / sd_delta,
  #        beta_diff = (beta - mean_beta ) / sd_beta,
  #        beta_0_diff = (beta_0 - mean_beta_0 ) / sd_beta_0,
  #        dictator_diff = (dictator - mean_dictator ) / sd_dictator,
  #        dictator_schoolmate_diff = (dictator_schoolmate - mean_dictator_schoolmate ) / sd_dictator_schoolmate,
  #        risk_diff = (risk - mean_risk ) / sd_risk,
  #        publicgood_diff = (publicgood - mean_publicgood ) / sd_publicgood,
  #        trust_diff = (trust - mean_trust ) / sd_trust,
  #        trust_return_diff = (trust_return - mean_trust_return ) / sd_trust_return,
  #        ) %>%
  # select(studid,classid,delta_diff,beta_diff,beta_0_diff,dictator_diff,dictator_schoolmate_diff,
  #        risk_diff,publicgood_diff,trust_diff,trust_return_diff)
  mutate(delta_diff = (delta / sd_delta) - mean_delta,
         beta_diff = (beta / sd_beta) - mean_beta ,
         beta_0_diff = (beta_0 / sd_beta_0) - mean_beta_0 ,
         dictator_diff = (dictator / sd_dictator) - mean_dictator ,
         dictator_schoolmate_diff = (dictator_schoolmate / sd_dictator_schoolmate) - mean_dictator_schoolmate ,
         risk_diff = (risk / sd_risk) - mean_risk,
         publicgood_diff = (publicgood / sd_publicgood) - mean_publicgood ,
         trust_diff = (trust / sd_trust) - mean_trust ,
         trust_return_diff = (trust_return / sd_trust_return) - mean_trust_return ,
         ) %>% 
  select(studid,classid,delta_diff,beta_diff,beta_0_diff,dictator_diff,dictator_schoolmate_diff,
         risk_diff,publicgood_diff,trust_diff,trust_return_diff) %>% 
ungroup()


```





### Defining the number of Principal Components to look at 


Using the demeaned data, the factor choice is no longer clear at factor = 4 but it is good enough




```{r, warning=FALSE,dpi=300, fig.width=10, fig.height=4}

pr.out <- pca_data_demeaned %>% select(-studid,-classid) %>% prcomp(., scale = TRUE)


scree_plot_data <- pca_data_demeaned %>%  select(-studid,-classid) %>% psych::principal( nfactors = 4, rotate= "varimax")

pve <- 100 * pr.out$sdev^2 / sum(pr.out$sdev ^2)
par(mfrow = c(1, 2))
plot(pve , type = "o", ylab = "PVE", main = "Scree-plot for Principal Component Analysis", frame = FALSE, xlab = "Principal Component", col = "blue")
```


Alternatively, look at the number of Principal Components to attaint using Parallel Analysis.

Parallel Analysis: specify how many components to retain using simulation

Here, we get the critical values for each component. If the eigenvalues from the PCA are greater than the value at 0.95 confint, we can retain given component.

We have two rules:
1) Kaiser's rule: Eigenvalue has to be >1 (satisfied up to 4 factors, with the 4th being barely above)
2) Eigenvalues being greater than the estimates gathered from Parallel Analysis (Horn, 1965)

```{r, warning=FALSE}


library(hornpa)
hornpa(k = 4, #test for number of factors
       size = nrow(pca_data_demeaned), # size of dataset
       reps = 500, # number of simulations
       seed = 1234) #set seed


scree_plot_data <- pca_data_demeaned %>%  select(-studid,-classid) %>% psych::principal( nfactors = 4, rotate= "varimax")
scree_plot_data



rm(scree_plot_data,pve)
```


Overall, no clear indication on the number of components; still, in order to make it comparable with the non-demeaned data, I calculate with 4 factors in the PCA.



### 3 Factor PCA



```{r warning=FALSE,dpi=300, fig.width=10, fig.height=4}

pca_varimax<- pca_data_demeaned %>%  select(-studid,-classid) %>% psych::principal( nfactors = 3, rotate= "varimax")



data.frame(head(pca_varimax$loadings, n=nrow(pca_varimax$loadings))) %>%
  mutate(variable = rownames(.)) %>%
  gather(component,loading,-variable) %>%
  ggplot(aes(loading,variable))+
  geom_col(fill = "midnightblue")+
  facet_wrap(~component,nrow=1)+
    labs(color = NULL,
       title = "Factor Loadings for each Component",
      subtitle ="With 3 factors",
      caption = "diff = [ y -avg(y)  / sd(y)] ")+
  ylab(NULL)+
  xlab(NULL)+
  theme_minimal()
```



#### 4 Factor PCA 





```{r warning=FALSE,dpi=300, fig.width=10, fig.height=4}

pca_varimax<- pca_data_demeaned %>%  select(-studid,-classid) %>% psych::principal( nfactors = 4, rotate= "varimax")


pca_varimax$loadings

data.frame(head(pca_varimax$loadings, n=nrow(pca_varimax$loadings))) %>%
  mutate(variable = rownames(.)) %>%
  gather(component,loading,-variable) %>%
  ggplot(aes(loading,variable))+
  geom_col(fill = "midnightblue")+
  facet_wrap(~component,nrow=1)+
    labs(color = NULL,
       title = "Factor Loadings for each Component",
      subtitle ="With 4 factors",
      caption = "diff = [ y -avg(y)  / sd(y)] ")+
  ylab(NULL)+
  xlab(NULL)+
  theme_minimal()
```


Bottom line: demeaned megmaradtak ugyanazok az eredmények.

A továbbiakban a 4 faktoros PCA-val számoltam. Az elemek elnevezései a következők:

- RC1.: Risky choice
- RC2: Riskless Choice
- RC3: Time inconsistency
- RC4: Time pref





```{r}


pca_varimax<- pca_data_demeaned %>%  select(-studid,-classid) %>% psych::principal( nfactors = 4, rotate= "varimax")


# RC1.: interakciót igénylő társas preferenciák
# RC2: időpreferencia
# RC4: Interakciót nem igénylő társas preferenciák
# RC3: kockázati preferencia




pca_data_demeaned$risky_choice <- pca_varimax$scores[,1]
pca_data_demeaned$riskless_choice <- pca_varimax$scores[,2]
pca_data_demeaned$time_inconsistency <- pca_varimax$scores[,4]
pca_data_demeaned$time_pref <- pca_varimax$scores[,3]





# add variables from the original data

additional_variables <- data %>%
select(studid,
        schoolid,
         academic,
         comp,
         FinalProfitRounded,
         math,
         read,
         female,
         male,
         age,
         gpa_i,
         grade_math,
         grade_hun,
         grade_lit)



data_analysis <- left_join(pca_data_demeaned,additional_variables, by ="studid")



data_analysis_clean <- data_analysis %>%
  # select(-delta, # I am removing all data that pca components include
  #        -beta,
  #        -beta_0,
  #        -dictator,
  #        -dictator_schoolmate,
  #        -risk,
  #        -publicgood,
  #        -trust,
  #        -trust_return)%>%
  drop_na() %>%
  mutate(good_stud = case_when(gpa_i>=4.75~1,
                               TRUE~0))



data_analysis_clean_clusteringdata <- data_analysis_clean %>% select(risky_choice,
                                                                     riskless_choice,
                                                                     time_inconsistency,
                                                                     time_pref)

```



```{r}

plot3d(
  x=pca_data_demeaned$risky_choice, y=pca_data_demeaned$riskless_choice, z=pca_data_demeaned$time_inconsistency,
  #col = data$color,
  #type = 's',
  radius = .1,
  xlab="Risky Choice.", ylab="Riskless Choice", zlab="Time inconsistency")

```











Az ÖSSZES további Random Forest illesztésben a "math" változóra illesztettem a fákat. Lehet még olyat is illeszteni, ahol pl. top 15% percentilisre lövünk.

Random forest math változóra; PCA változók nélkül



```{r warning=FALSE, dpi=300, fig.width=14, fig.height=8}

set.seed(425643)

rf_rpart <- rpart::rpart(math~delta_diff+beta_diff+dictator_diff+dictator_schoolmate_diff+risk_diff+publicgood_diff+
                           trust_diff+trust_return_diff+
                           comp, data=data_analysis_clean)


# to plot this, we need to make this compatible with the "party" package


rf_rpart2 <- partykit::as.party(rf_rpart)
plot(rf_rpart2)
```

<!-- Összehasonlítás a nyers változókkal: -->

<!-- ```{r warning=FALSE,dpi=300, fig.width=14, fig.height=4} -->

<!-- set.seed(4286) -->


<!-- data_nomiss <- data %>%  -->
<!--           dplyr::select(math,delta,beta,dictator,dictator_schoolmate,risk,publicgood, -->
<!--                            trust,trust_return, comp) %>%  -->
<!--   drop_na() -->

<!-- rf_rpart <- rpart::rpart(math~delta+beta+dictator+dictator_schoolmate+risk+publicgood+ -->
<!--                            trust+trust_return+ -->
<!--                            comp, data=data_nomiss) -->


<!-- # to plot this, we need to make this compatible with the "party" package -->


<!-- rf_rpart2 <- partykit::as.party(rf_rpart) -->
<!-- plot(rf_rpart2) -->


<!-- rm(data_nomiss) -->
<!-- ``` -->

<!-- ####Az összehasonlítás eredménye: -->

<!-- <!-- A demeanelt verzió sokkal jobban szétszedi a "klasztereket". mintha a nyers változókkal számolunk (köszönhetően annak, hogy kvázi normalizáltuk az adatot) --> -->

<!-- <!-- Legjobb teljesítményűek (azaz ahol a klaszterek átlaga 1 körül van): --> -->

<!-- <!-- Demeanelt esetben: (összesen kb. 100 megfigyelés) --> -->
<!-- <!-- - Node 7 : <0.5 publicgood_diff és 0.28<=dictator_diff<0.66 (csak ez a két változó érdekes ennél) --> -->
<!-- <!-- - Node 24 : >=0.526 public_good <1.12 ; trust_diff <0.5 és delta_diff >=0.66 --> -->
<!-- <!-- - Node 27 : >=0.526 public_good <1.12 ; trust_diff >=0.5 (itt válik el az előzőtől) és trust_diff <1.123 --> -->


<!-- <!-- Sima változók esetében: (összesen kb. 130 megfigyelés) --> -->
<!-- <!-- - Node 14 : delta >=0.78; publicgood <98.65 és dictator < 1.5 --> -->
<!-- <!-- - Node 18 : delta >=0.78; publicgood >=98.65; trust <75 ; dictator_schoolmate <3.75 --> -->
<!-- <!-- - Node 19 : delta >=0.78; publicgood >=98.65; trust >=75 (ez a három közül a legnagyobb) --> -->

<!-- <!-- Tehát a publig good és a trust mindig nagyon fontos, nyers adatoknál pedig a delta. --> -->
<!-- <!-- Fontos még a dictator game is --> -->






<!-- ##Klasszifikáció a legjobb 20% matekosra -->


<!-- Ha a legjobb 20% matekosra futtatok klasszifikációt, akkor a legfontosabbak a klasszifikáció szempontjából a következők: -->

<!-- - trust -->
<!-- - delta -->
<!-- - trust_return -->
<!-- - fontos még a dictator_schoolmate (de csak 33 eset klasszifikációjában ebben a futtatásban) -->


```{r warning=FALSE,dpi=300, fig.width=14, fig.height=8}

set.seed(4286)


data_nomiss <- data %>% 
          dplyr::select(math,delta,beta,dictator,dictator_schoolmate,risk,publicgood,
                           trust,trust_return, comp) %>% 
  drop_na() %>% 
  mutate(math_good = case_when(math >= quantile(math, probs = c(0.8)) ~1,
                                TRUE~0))

rf_rpart <- rpart::rpart(math_good~delta+beta+dictator+dictator_schoolmate+risk+publicgood+
                           trust+trust_return+
                           comp, data=data_nomiss)


# to plot this, we need to make this compatible with the "party" package


rf_rpart2 <- partykit::as.party(rf_rpart)
plot(rf_rpart2)


rm(data_nomiss)
```








------------------


<!-- Random Forest PCA-kkal: -->

<!-- ```{r warning=FALSE,dpi=300, fig.width=10, fig.height=4} -->

<!-- set.seed(425643231) -->

<!-- rf_rpart <- rpart::rpart(math~pca_socpref_interact+pca_timepref+ -->
<!--                               pca_socpref_nointeract+pca_riskpref, data=data_analysis_clean) -->


<!-- # to plot this, we need to make this compatible with the "party" package -->


<!-- rf_rpart2 <- partykit::as.party(rf_rpart) -->
<!-- plot(rf_rpart2) -->
<!-- ``` -->


<!-- Mivel jóval kevesebb változó van, kevésbé tudja szétszedni a jókat.  -->

<!-- Összességében: -->
<!-- - A "Node 9"-ben vannak a jobb teljesítményű diákok (n=130) : social_preferencbe nagyon alacsony a score-juk, time preferencbe pedig magasabb. -->





<!-- # Clustering - demeanelt adatokra -->


<!--  (Mindegyik esetben 5 klaszternél húztam meg a határt. ) -->





<!-- For the hierarchical clusters, in order to get this amount of clusters, we chose a cut-off value of 8.1. -->

```{r}
raw_clustering <- data_analysis_clean %>% select(delta_diff,beta_diff,beta_0_diff,                                                 dictator_diff,dictator_schoolmate_diff,risk_diff,publicgood_diff,
                                                 trust_diff,trust_return_diff)
```



<!-- Complete clustering: -->

<!-- ```{r} -->
<!-- hc.complete <- hclust(dist(raw_clustering), method = "complete") -->
<!-- dendagram <- as.dendrogram(hc.complete) -->

<!-- LAB = rep("", nobs(dendagram)) -->
<!-- dendagram = dendextend::set(dendagram, "labels", LAB) -->

<!-- plot(dendextend::color_branches(dendagram, k = 5), main="Hierarchical clusters ['Complete' method]", sub ="5 clusters with cut-off value 8.1", leaflab = "none", horiz = F) -->
<!-- abline(h = 8.0, col = 'red') -->


<!-- # save the clusters: -->

<!-- # hierarchical clustering: -->
<!-- cut_complete <- cutree(hc.complete, h=8.0) -->



<!-- clustered_data <- raw_clustering -->

<!-- clustered_data$hierarchical_cluster <- cut_complete -->


<!-- ``` -->



<!-- Average Hierarchical clustering: -->

<!-- ```{r} -->
<!-- set.seed(1300) -->


<!-- hc.complete <- hclust(dist(raw_clustering), method = "mcquitty") -->
<!-- dendagram <- as.dendrogram(hc.complete) -->

<!-- LAB = rep("", nobs(dendagram)) -->
<!-- dendagram = dendextend::set(dendagram, "labels", LAB) -->

<!-- plot(dendextend::color_branches(dendagram, k = 7), main="Hierarchical clusters ['McQuitty' method]", sub ="5 clusters with cut-off value 5.4 (two outlier groups)", leaflab = "none", horiz = F) -->
<!-- abline(h = 5.4, col = 'red') -->

<!-- cut_complete <- cutree(hc.complete, h=5.4) -->

<!-- #replace outliers to their closest groups -->

<!-- # NOTE : this is done manually by checking which clusters are the closest -->

<!-- cut_complete[cut_complete == 6 ] <- 4 -->
<!-- cut_complete[cut_complete == 7 ] <- 5 -->




<!-- clustered_data$hierarchical_mcquitty_cluster <- cut_complete -->


<!-- ``` -->





# K-means clustering: [ Egyelőre ez az egyetlen, amit megcsináltam erre ]

As a first step, I check the optimal number of clusters using the elbow-plot; the results show that 5 is optimal, which is in line with the hierarchical clustering method.

```{r, warning = FALSE}
# elbow method:
set.seed(123)

# function to compute total within-cluster sum of square
wss <- function(k) {
  kmeans(raw_clustering, k, nstart = 100)$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 1:50

# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)

plot(k.values, wss_values,
       type="b", pch = 19, frame = FALSE,
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")
```
Now knowing that the number of factors is the same, I run the K-means clustering algorithm for 5 clusters:

```{r, warning = FALSE}
set.seed(123469)
k5 <- kmeans(raw_clustering, centers = 5, nstart = 10000)


fviz_cluster(k5, geom = "point",  data = raw_clustering) + ggtitle("Clusters with 5 centers and 10,000 simulations")+
  theme_minimal()


# get classes:

#k-means:
 # clustered_data$kmeans_cluster <- k5$cluster

```




<!-- #### Cross-table of various observations using the two clustering algorithms: -->


<!-- ##### Complete Hierarchical vs K-means: -->

<!-- ```{r} -->
<!--  table(clustered_data$hierarchical_cluster,clustered_data$kmeans_cluster) %>% -->
<!--    kbl()%>% -->
<!--    kable_minimal() -->
<!-- ``` -->


<!-- # ##### Complete Hierarchical vs McQuitty Hierarchical: -->


<!-- ```{r} -->
<!--  table(clustered_data$hierarchical_cluster,clustered_data$hierarchical_mcquitty_cluster) %>% -->
<!--    kbl() %>% -->
<!--    kable_minimal() -->
<!-- ``` -->


<!-- # ##### McQuitty Hierarchical vs K-means: -->

<!-- ```{r} -->
<!-- # table(clustered_data$hierarchical_mcquitty_cluster,clustered_data$kmeans_cluster) %>% -->
<!-- #   kbl()%>% -->
<!-- #   kable_minimal() -->
<!-- ``` -->




