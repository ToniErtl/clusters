---
title: "demeaned_clustering"
author: "Antal Ertl"
date: '2023 03 01 '
output:
  html_document
---

```{r setup, include=FALSE}


knitr::opts_chunk$set(echo = FALSE) # not include the code

library(tidyverse)
#library(tidymodels)
library(ggplot2)
library(ggthemes)

library(rgl) # 3 dimensional plots
library(factoextra)
library(crosstable) # crosstable function


# Clustering methods:
library(cluster)

# For random forest modeling and visualization:
library(rpart)
library(partykit)
library(party)

# Additional packages for html output:

library(knitr)
library(kableExtra)

# -----------------------------# 



# Functions used: 

corrplot2 <- function(data,
                      method = "pearson",
                      sig.level = 0.05,
                      order = "original",
                      diag = FALSE,
                      type = "upper",
                      tl.srt = 90,
                      number.font = 1,
                      number.cex = 1,
                      mar = c(0, 0, 0, 0)) {
  library(corrplot)
  data_incomplete <- data
  data <- data[complete.cases(data), ]
  mat <- cor(data, method = method)
  cor.mtest <- function(mat, method) {
    mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat <- matrix(NA, n, n)
    diag(p.mat) <- 0
    for (i in 1:(n - 1)) {
      for (j in (i + 1):n) {
        tmp <- cor.test(mat[, i], mat[, j], method = method)
        p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
      }
    }
    colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
    p.mat
  }
  p.mat <- cor.mtest(data, method = method)
  col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
  corrplot(mat,
    method = "color", col = col(200), number.font = number.font,
    mar = mar, number.cex = number.cex,
    type = type, order = order,
    addCoef.col = "black", # add correlation coefficient
    tl.col = "black", tl.srt = tl.srt, # rotation of text labels
    # combine with significance level
    p.mat = p.mat, sig.level = sig.level, insig = "blank",
    # hide correlation coefficiens on the diagonal
    diag = diag
  )
}

```

```{r load data}
data <- read.csv(".//Data in brief//Horn-Kiss-Lenard2021.csv")
```

<!-- demeanelni osztályonként az összes score-t: -->

<!-- - classid-nként megnézni az átlagot és ahhoz képest mennyi standard eloszlásra van az adott ember adott scoreból -->

<!-- ezekre a deamenelt scoreokra futtatni az egészet -->

<!-- osztály átlag hisztogramok az egyes nem demeanelt változókra. -->

<!-- Április 20-21 -->

---------------------------------------------


```{r}

pca_data <- data %>%  mutate(STournamentChoosed =replace_na(STournamentChoosed,0)) %>%
        mutate(STournamentChoosed_comp =STournamentChoosed* comp) %>%
                     dplyr::select(studid,
                                   classid,
                                   delta,
                                   #delta_now,
                                   beta,
                                   #pb,
                                   beta_0, # time inconsistency
                                   dictator, #altruism
                                   dictator_schoolmate, #altruism
                                   risk, # risk
                                   publicgood, # cooperativeness
                                   trust, # trust
                                   trust_return) %>%  #trust
  drop_na() 



# demean: redefine variable where we indicate for each individual how many standard deviations he or she is from the class average

pca_data_demeaned<- data %>%  mutate(STournamentChoosed =replace_na(STournamentChoosed,0)) %>%
                    mutate(STournamentChoosed_comp =STournamentChoosed* comp) %>%
                     dplyr::select(studid,
                                   classid,
                                   delta,
                                   #delta_now,
                                   beta,
                                   #pb,
                                   beta_0, # time inconsistency
                                   dictator, #altruism
                                   dictator_schoolmate, #altruism
                                   risk, # risk
                                   publicgood, # cooperativeness
                                   trust, # trust
                                   trust_return) %>%  #trust
  drop_na() %>% 
  group_by(classid) %>% 
  mutate(#mean values: 
         mean_delta= mean(delta),
         mean_beta = mean(beta),
         mean_beta_0 = mean(beta_0),
         mean_dictator = mean(dictator),
         mean_dictator_schoolmate = mean(dictator_schoolmate),
         mean_risk = mean(risk),
         mean_publicgood = mean(publicgood),
         mean_trust = mean(trust),
         mean_trust_return = mean(trust_return),
         #standard deviations:
         sd_delta = sd(delta),
         sd_beta = sd(beta),
         sd_beta_0 = sd(beta_0),
         sd_dictator = sd(dictator),
         sd_dictator_schoolmate = sd(dictator_schoolmate),
         sd_risk = sd(risk),
         sd_publicgood = sd(publicgood),
         sd_trust = sd(trust),
         sd_trust_return = sd(trust_return)) %>% 
  ungroup() %>% 
  # calculate how many standard deviations one observation is from the mean
  mutate(delta_diff = (delta - mean_delta ) / sd_delta,
         beta_diff = (beta - mean_beta ) / sd_beta,
         beta_0_diff = (beta_0 - mean_beta_0 ) / sd_beta_0,
         dictator_diff = (dictator - mean_dictator ) / sd_dictator,
         dictator_schoolmate_diff = (dictator_schoolmate - mean_dictator_schoolmate ) / sd_dictator_schoolmate,
         risk_diff = (risk - mean_risk ) / sd_risk,
         publicgood_diff = (publicgood - mean_publicgood ) / sd_publicgood,
         trust_diff = (trust - mean_trust ) / sd_trust,
         trust_return_diff = (trust_return - mean_trust_return ) / sd_trust_return,
         ) %>%
  select(studid,classid,delta_diff,beta_diff,beta_0_diff,dictator_diff,dictator_schoolmate_diff,
         risk_diff,publicgood_diff,trust_diff,trust_return_diff)
  # mutate(delta_diff = (delta / sd_delta) - mean_delta,
  #        beta_diff = (beta / sd_beta) - mean_beta ,
  #        beta_0_diff = (beta_0 / sd_beta_0) - mean_beta_0 ,
  #        dictator_diff = (dictator / sd_dictator) - mean_dictator ,
  #        dictator_schoolmate_diff = (dictator_schoolmate / sd_dictator_schoolmate) - mean_dictator_schoolmate ,
  #        risk_diff = (risk / sd_risk) - mean_risk,
  #        publicgood_diff = (publicgood / sd_publicgood) - mean_publicgood ,
  #        trust_diff = (trust / sd_trust) - mean_trust ,
  #        trust_return_diff = (trust_return / sd_trust_return) - mean_trust_return ,
  #        ) %>%
  # select(studid,classid,delta_diff,beta_diff,beta_0_diff,dictator_diff,dictator_schoolmate_diff,
  #        risk_diff,publicgood_diff,trust_diff,trust_return_diff)


```




### Defining the number of Principal Components to look at 


Using the demeaned data, the factor choice is no longer clear at factor = 4 but it is good enough




```{r, warning=FALSE,dpi=300, fig.width=10, fig.height=4}

pr.out <- pca_data_demeaned %>% select(-studid,-classid) %>% prcomp(., scale = TRUE)


scree_plot_data <- pca_data_demeaned %>%  select(-studid,-classid) %>% psych::principal( nfactors = 4, rotate= "varimax")

pve <- 100 * pr.out$sdev^2 / sum(pr.out$sdev ^2)
par(mfrow = c(1, 2))
plot(pve , type = "o", ylab = "PVE", main = "Scree-plot for Principal Component Analysis", frame = FALSE, xlab = "Principal Component", col = "blue")
```


Alternatively, look at the number of Principal Components to attaint using Parallel Analysis.

Parallel Analysis: specify how many components to retain using simulation

Here, we get the critical values for each component. If the eigenvalues from the PCA are greater than the value at 0.95 confint, we can retain given component.

We have two rules:
1) Kaiser's rule: Eigenvalue has to be >1 (satisfied up to 4 factors, with the 4th being barely above)
2) Eigenvalues being greater than the estimates gathered from Parallel Analysis (Horn, 1965)

```{r, warning=FALSE}


library(hornpa)
hornpa(k = 4, #test for number of factors
       size = nrow(pca_data_demeaned), # size of dataset
       reps = 500, # number of simulations
       seed = 1234) #set seed


scree_plot_data <- pca_data_demeaned %>%  select(-studid,-classid) %>% psych::principal( nfactors = 4, rotate= "varimax")
scree_plot_data



rm(scree_plot_data,pve)
```


Overall, no clear indication on the number of components; still, in order to make it comparable with the non-demeaned data, I calculate with 4 factors in the PCA.



### 3 Factor PCA



```{r warning=FALSE,dpi=300, fig.width=10, fig.height=4}

pca_varimax<- pca_data_demeaned %>%  select(-studid,-classid) %>% psych::principal( nfactors = 3, rotate= "varimax")



data.frame(head(pca_varimax$loadings, n=nrow(pca_varimax$loadings))) %>%
  mutate(variable = rownames(.)) %>%
  gather(component,loading,-variable) %>%
  ggplot(aes(loading,variable))+
  geom_col(fill = "midnightblue")+
  facet_wrap(~component,nrow=1)+
    labs(color = NULL,
       title = "Factor Loadings for each Component",
      subtitle ="With 3 factors",
      caption = "diff = [ y -avg(y)  / sd(y)] ")+
  ylab(NULL)+
  xlab(NULL)+
  theme_minimal()
```



#### 4 Factor PCA 





```{r warning=FALSE,dpi=300, fig.width=10, fig.height=4}

pca_varimax<- pca_data_demeaned %>%  select(-studid,-classid) %>% psych::principal( nfactors = 4, rotate= "varimax")


pca_varimax$loadings

data.frame(head(pca_varimax$loadings, n=nrow(pca_varimax$loadings))) %>%
  mutate(variable = rownames(.)) %>%
  gather(component,loading,-variable) %>%
  ggplot(aes(loading,variable))+
  geom_col(fill = "midnightblue")+
  facet_wrap(~component,nrow=1)+
    labs(color = NULL,
       title = "Factor Loadings for each Component",
      subtitle ="With 4 factors",
      caption = "diff = [ y -avg(y)  / sd(y)] ")+
  ylab(NULL)+
  xlab(NULL)+
  theme_minimal()
```


Bottom line: demeaned megmaradtak ugyanazok az eredmények.

A továbbiakban a 4 faktoros PCA-val számoltam. Az elemek elnevezései a következők:

- RC1.: interakciót igénylő társas preferenciák
- RC2: időpreferencia
- RC4: Interakciót nem igénylő társas preferenciák
- RC3: kockázati preferencia


--------------------


```{r}


pca_varimax<- pca_data_demeaned %>%  select(-studid,-classid) %>% psych::principal( nfactors = 4, rotate= "varimax")


# RC1.: interakciót igénylő társas preferenciák
# RC2: időpreferencia
# RC4: Interakciót nem igénylő társas preferenciák
# RC3: kockázati preferencia




pca_data_demeaned$pca_socpref_interact <- pca_varimax$scores[,1]
pca_data_demeaned$pca_timepref <- pca_varimax$scores[,2]
pca_data_demeaned$pca_socpref_nointeract <- pca_varimax$scores[,4]
pca_data_demeaned$pca_riskpref <- pca_varimax$scores[,3]





# add variables from the original data

additional_variables <- data %>%
select(studid,
        schoolid,
         academic,
         comp,
         FinalProfitRounded,
         math,
         read,
         female,
         male,
         age,
         gpa_i,
         grade_math,
         grade_hun,
         grade_lit)



data_analysis <- left_join(pca_data_demeaned,additional_variables, by ="studid")



data_analysis_clean <- data_analysis %>%
  # select(-delta, # I am removing all data that pca components include
  #        -beta,
  #        -beta_0,
  #        -dictator,
  #        -dictator_schoolmate,
  #        -risk,
  #        -publicgood,
  #        -trust,
  #        -trust_return)%>%
  drop_na() %>%
  mutate(good_stud = case_when(gpa_i>=4.75~1,
                               TRUE~0))



data_analysis_clean_clusteringdata <- data_analysis_clean %>% select(pca_socpref_interact,
                                                                     pca_timepref,
                                                                     pca_socpref_nointeract,
                                                                     pca_riskpref)

```



```{r}

plot3d(
  x=pca_data_demeaned$pca_socpref_interact, y=pca_data_demeaned$pca_timepref, z=pca_data_demeaned$pca_riskpref,
  #col = data$color,
  #type = 's',
  radius = .1,
  xlab="Soc. Pref. Interact.", ylab="Time Pref.", zlab="Soc. Pref. No-Interact.")

```










## Random Forest


Az ÖSSZES további Random Forest illesztésben a "math" változóra illesztettem a fákat. Lehet még olyat is illeszteni, ahol pl. top 15% percentilisre lövünk.

Random forest math változóra; PCA változók nélkül



```{r warning=FALSE,dpi=300, fig.width=14, fig.height=4}

set.seed(425643)

rf_rpart <- rpart::rpart(math~delta_diff+beta_diff+dictator_diff+dictator_schoolmate_diff+risk_diff+publicgood_diff+
                           trust_diff+trust_return_diff+
                           comp, data=data_analysis_clean)


# to plot this, we need to make this compatible with the "party" package


rf_rpart2 <- partykit::as.party(rf_rpart)
plot(rf_rpart2)
```

Összehasonlítás a nyers változókkal:

```{r warning=FALSE,dpi=300, fig.width=14, fig.height=4}

set.seed(4286)


data_nomiss <- data %>% 
          dplyr::select(math,delta,beta,dictator,dictator_schoolmate,risk,publicgood,
                           trust,trust_return, comp) %>% 
  drop_na()

rf_rpart <- rpart::rpart(math~delta+beta+dictator+dictator_schoolmate+risk+publicgood+
                           trust+trust_return+
                           comp, data=data_nomiss)


# to plot this, we need to make this compatible with the "party" package


rf_rpart2 <- partykit::as.party(rf_rpart)
plot(rf_rpart2)


rm(data_nomiss)
```

#### Az összehasonlítás eredménye:

<!-- A demeanelt verzió sokkal jobban szétszedi a "klasztereket". mintha a nyers változókkal számolunk (köszönhetően annak, hogy kvázi normalizáltuk az adatot) -->

<!-- Legjobb teljesítményűek (azaz ahol a klaszterek átlaga 1 körül van): -->

<!-- Demeanelt esetben: (összesen kb. 100 megfigyelés) -->
<!-- - Node 7 : <0.5 publicgood_diff és 0.28<=dictator_diff<0.66 (csak ez a két változó érdekes ennél) -->
<!-- - Node 24 : >=0.526 public_good <1.12 ; trust_diff <0.5 és delta_diff >=0.66 -->
<!-- - Node 27 : >=0.526 public_good <1.12 ; trust_diff >=0.5 (itt válik el az előzőtől) és trust_diff <1.123 -->


<!-- Sima változók esetében: (összesen kb. 130 megfigyelés) -->
<!-- - Node 14 : delta >=0.78; publicgood <98.65 és dictator < 1.5 -->
<!-- - Node 18 : delta >=0.78; publicgood >=98.65; trust <75 ; dictator_schoolmate <3.75 -->
<!-- - Node 19 : delta >=0.78; publicgood >=98.65; trust >=75 (ez a három közül a legnagyobb) -->

<!-- Tehát a publig good és a trust mindig nagyon fontos, nyers adatoknál pedig a delta. -->
<!-- Fontos még a dictator game is -->




## Sima lineáris regresszió - PCA-kkal


Innen is nagyjából az jön ki, mint az előző pontból: ha a matekra illesztek egy sima lineáris regressziót, akkor a pca_socpref_interact marad szignifikáns ha berakom a kort és az olvasást. Ha ezeket kihagyom, akkor a másik social preference és a timepreference is szignifikáns lesz. 

```{r}

summary(lm(math~pca_socpref_interact+pca_timepref+pca_socpref_nointeract+pca_riskpref+comp+age+read, data= data_analysis_clean))

```

A két regresszió közti R-négyzet különbség is nagyon durva.



```{r}
summary(lm(math~pca_socpref_interact+pca_timepref+pca_socpref_nointeract+pca_riskpref+comp, data= data_analysis_clean))
```



diff adatokra illesztve:

```{r}
summary(lm(math~delta_diff+beta_diff+dictator_diff+dictator_schoolmate_diff+risk_diff+publicgood_diff+
                           trust_diff+trust_return_diff+age+read, data= data_analysis_clean))
```



## Klasszifikáció a legjobb 20% matekosra


Ha a legjobb 20% matekosra futtatok klasszifikációt, akkor a legfontosabbak a klasszifikáció szempontjából a következők:

- trust
- delta
- trust_return
- fontos még a dictator_schoolmate (de csak 33 eset klasszifikációjában ebben a futtatásban)


```{r warning=FALSE,dpi=300, fig.width=14, fig.height=4}

set.seed(4286)


data_nomiss <- data %>% 
          dplyr::select(math,delta,beta,dictator,dictator_schoolmate,risk,publicgood,
                           trust,trust_return, comp) %>% 
  drop_na() %>% 
  mutate(math_good = case_when(math >= quantile(math, probs = c(0.8)) ~1,
                                TRUE~0))

rf_rpart <- rpart::rpart(math_good~delta+beta+dictator+dictator_schoolmate+risk+publicgood+
                           trust+trust_return+
                           comp, data=data_nomiss)


# to plot this, we need to make this compatible with the "party" package


rf_rpart2 <- partykit::as.party(rf_rpart)
plot(rf_rpart2)


rm(data_nomiss)
```








------------------


Random Forest PCA-kkal:

```{r warning=FALSE,dpi=300, fig.width=10, fig.height=4}

set.seed(425643231)

rf_rpart <- rpart::rpart(math~pca_socpref_interact+pca_timepref+
                              pca_socpref_nointeract+pca_riskpref, data=data_analysis_clean)


# to plot this, we need to make this compatible with the "party" package


rf_rpart2 <- partykit::as.party(rf_rpart)
plot(rf_rpart2)
```


Mivel jóval kevesebb változó van, kevésbé tudja szétszedni a jókat. 

Összességében:
- A "Node 9"-ben vannak a jobb teljesítményű diákok (n=130) : social_preferencbe nagyon alacsony a score-juk, time preferencbe pedig magasabb.






# Random forest - read


"read változóra megcsinálom az előző elemezést:




```{r warning=FALSE, dpi=300, fig.width=14, fig.height=4}

set.seed(425643)

rf_rpart <- rpart::rpart(read~delta_diff+beta_diff+dictator_diff+dictator_schoolmate_diff+risk_diff+publicgood_diff+
                           trust_diff+trust_return_diff+
                           comp, data=data_analysis_clean)


# to plot this, we need to make this compatible with the "party" package


rf_rpart2 <- partykit::as.party(rf_rpart)
plot(rf_rpart2)
```

Összehasonlítás a nyers változókkal:

```{r warning=FALSE,dpi=300, fig.width=14, fig.height=4}

set.seed(4286)


data_nomiss <- data %>% 
          dplyr::select(read,delta,beta,dictator,dictator_schoolmate,risk,publicgood,
                           trust,trust_return, comp) %>% 
  drop_na()

rf_rpart <- rpart::rpart(read~delta+beta+dictator+dictator_schoolmate+risk+publicgood+
                           trust+trust_return+
                           comp, data=data_nomiss)


# to plot this, we need to make this compatible with the "party" package


rf_rpart2 <- partykit::as.party(rf_rpart)
plot(rf_rpart2)


rm(data_nomiss)
```

#### Az összehasonlítás eredménye readre

A demeanelt verzió sokkal jobban szétszedi a "klasztereket", csakúgy, mint a math esetében.

Legjobb teljesítményűek (azaz ahol a klaszterek átlaga 1 körül van):

Demeanelt esetben: (összesen kb. 100 megfigyelés, csakúgy, mint math esetében)
- Node 7  (N=7; nagyon kicsi klaszter) : <-2.235 publicgood_diff (EXTRÉM alacsony), beta_diff <0.43 és delta_diff < 1.324 (egész magas értékűek)
- Node 21 (N = 97 - ez lényegesen nagyobb klaszter!): 0.43=< publicgood_diff <1.65 és 0.68=<trust_diff<1.27 : EZEK LEHETNEK ESETLEG "RECIPROCITÁS"-CSOPORT?

Sima változók esetében: (összesen kb megint 100 megfigyelés):

- Node 8 (N= 45 ): publicgood és risk (risk tulajdonképpen a beta és delta_diff helyett van az előző fa node 7-éhez képest)
- Node 13 (N = 61): publicgood >=71.25, trust >= 65 és trust_return <35.06 : EZ IS VALAMI HASONLÓ, RECIPROCITÁS /EGALITÁRIÁNUS CSOPORT?






### Sima lineáris regresszió - PCA-kkal


Nagyon érdekes: a matekkal ellentétben itt a socpref_nointeract ami dominál (dictator game)

```{r}
summary(lm(read~pca_socpref_interact+pca_timepref+pca_socpref_nointeract+pca_riskpref+comp+age+math, data= data_analysis_clean))
```

diff adatokra illesztve:

```{r}
summary(lm(read~delta_diff+beta_diff+dictator_diff+dictator_schoolmate_diff+risk_diff+publicgood_diff+
                           trust_diff+trust_return_diff+age, data= data_analysis_clean))
```


```{r}
summary(lm(read~delta+beta+dictator+dictator_schoolmate+risk+publicgood+
                           trust+trust_return+age, data= data))
```





















# Clustering - demeanelt adatokra


 (Mindegyik esetben 5 klaszternél húztam meg a határt. )





For the hierarchical clusters, in order to get this amount of clusters, we chose a cut-off value of 8.1.

```{r}
raw_clustering <- data_analysis_clean %>% select(delta_diff,beta_diff,beta_0_diff,                                                 dictator_diff,dictator_schoolmate_diff,risk_diff,publicgood_diff,
                                                 trust_diff,trust_return_diff)
```



Complete clustering:

```{r}
hc.complete <- hclust(dist(raw_clustering), method = "complete")
dendagram <- as.dendrogram(hc.complete)

LAB = rep("", nobs(dendagram))
dendagram = dendextend::set(dendagram, "labels", LAB)

plot(dendextend::color_branches(dendagram, k = 5), main="Hierarchical clusters ['Complete' method]", sub ="5 clusters with cut-off value 8.1", leaflab = "none", horiz = F)
abline(h = 8.0, col = 'red')


# save the clusters:

# hierarchical clustering:
cut_complete <- cutree(hc.complete, h=8.0)



clustered_data <- raw_clustering

clustered_data$hierarchical_cluster <- cut_complete


```



Average Hierarchical clustering:

```{r}
set.seed(1300)


hc.complete <- hclust(dist(raw_clustering), method = "mcquitty")
dendagram <- as.dendrogram(hc.complete)

LAB = rep("", nobs(dendagram))
dendagram = dendextend::set(dendagram, "labels", LAB)

plot(dendextend::color_branches(dendagram, k = 7), main="Hierarchical clusters ['McQuitty' method]", sub ="5 clusters with cut-off value 5.4 (two outlier groups)", leaflab = "none", horiz = F)
abline(h = 5.4, col = 'red')

cut_complete <- cutree(hc.complete, h=5.4)

#replace outliers to their closest groups

# NOTE : this is done manually by checking which clusters are the closest

cut_complete[cut_complete == 6 ] <- 4
cut_complete[cut_complete == 7 ] <- 5




clustered_data$hierarchical_mcquitty_cluster <- cut_complete


```





K-means clustering:

As a first step, I check the optimal number of clusters using the elbow-plot; the results show that 5 is optimal, which is in line with the hierarchical clustering method.

```{r, warning = FALSE}
# elbow method:
set.seed(123)

# function to compute total within-cluster sum of square
wss <- function(k) {
  kmeans(raw_clustering, k, nstart = 100)$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 1:50

# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)

plot(k.values, wss_values,
       type="b", pch = 19, frame = FALSE,
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")
```
Now knowing that the number of factors is the same, I run the K-means clustering algorithm for 5 clusters:

```{r, warning = FALSE}
set.seed(123469)
k5 <- kmeans(raw_clustering, centers = 5, nstart = 10000)


fviz_cluster(k5, geom = "point",  data = raw_clustering) + ggtitle("Clusters with 5 centers and 10,000 simulations")+
  theme_minimal()


# get classes:

#k-means:
clustered_data$kmeans_cluster <- k5$cluster

```



### k-medoid (PAM):

```{r}
set.seed(11567)


summary(fpc::pamk(raw_clustering, krange = 3:10,
         metric = "euclidean",
         stand = TRUE )) # data is standadized; this needs to be true

kmed5<- cluster::pam(raw_clustering, k= 5,
         metric = "euclidean",
         stand = TRUE ) # data is standadized; this needs to be true

fviz_cluster(kmed5, geom = "point",  data = raw_clustering) + ggtitle("K-medoid clusters with 5 centers and 10,000 simulations")+
  theme_minimal()

```









#MCLUST clustering : (model based clustering)


(for more on that: https://cran.r-project.org/web/packages/mclust/vignettes/mclust.html)



```{r}
library(mclust)

set.seed(38912)


# check various model variations, which one is the best using the Bayesian Information Criteria (BIC):

BIC <- mclust::mclustBIC(data = raw_clustering)

mclust::plot.mclustBIC(BIC)

```

```{r}

# fit the model based on the best model specification:

summary(mod_mclust <- Mclust(raw_clustering, x = BIC), parameters = TRUE)

mod_mclust <- Mclust(raw_clustering, x = BIC)

mclust::plot.Mclust(mod_mclust, what = "classification")
```




#### Cross-table of various observations using the two clustering algorithms:


##### Complete Hierarchical vs K-means:

```{r}
table(clustered_data$hierarchical_cluster,clustered_data$kmeans_cluster) %>%
  kbl(row.names = TRUE,
      caption  = "Hierarchical clusters (row) vs. k-means cluster (columns)")%>%
  kable_minimal()
```


##### Complete Hierarchical vs McQuitty Hierarchical:


```{r}
table(clustered_data$hierarchical_cluster,clustered_data$hierarchical_mcquitty_cluster) %>%
  kbl() %>% 
  kable_minimal()
```


##### McQuitty Hierarchical vs K-means:

```{r}
table(clustered_data$hierarchical_mcquitty_cluster,clustered_data$kmeans_cluster) %>%
  kbl(row.names = TRUE,
      caption  = "Hierarchical clusters [McQuitty] (row) vs. k-means cluster (columns)")%>%
  kable_minimal()
```

Összehasonlítás summary:

(k-mean cluster szempontjából) az 1-es és 2-es klaszterek nagyon hasonló, a többiben szórnak a klaszterezési metódusok. 

Ezek kevesebb megfigyeléssel rendelkeznek, mint a többi, jobban el is válnak. Ezeket így külön meg is nézem





```{r adatok újracsatolása, inlcude = FALSE}


nyers_adatok <- data %>%  dplyr::select(studid,
                                   delta,
                                   #delta_now,
                                   beta,
                                   #pb,
                                   beta_0, # time inconsistency
                                   dictator, #altruism
                                   dictator_schoolmate, #altruism
                                   risk, # risk
                                   publicgood, # cooperativeness
                                   trust, # trust
                                   trust_return)


data_analysis_clean$kmeans_cluster <- clustered_data$kmeans_cluster
data_analysis_clean$hierarchical_cluster <- clustered_data$hierarchical_cluster

data_analysis_clean <- left_join(data_analysis_clean, nyers_adatok, by = "studid")


rm(nyers_adatok)

data_analysis_clean_describe <- data_analysis_clean %>%  dplyr::select(studid,
                                                                     classid,
                                                                     schoolid,
                                   delta,
                                   #delta_now,
                                   beta,
                                   #pb,
                                   #beta_0, # time inconsistency
                                   dictator, #altruism
                                   dictator_schoolmate, #altruism
                                   risk, # risk
                                   publicgood, # cooperativeness
                                   trust, # trust
                                   trust_return,
                                   comp,
                                   pca_socpref_interact,
                                   pca_timepref,
                                   pca_socpref_nointeract,
                                   pca_riskpref,
                                   math,
                                   read,
                                   age,
                                   kmeans_cluster)


```
 




# Clusters 1 and 2





<!-- ```{r warning=FALSE,dpi=300, fig.width=18, fig.height=20} -->

<!-- # clustered hozzarendelese az osszes valtozohoz -->

<!-- data_analysis_clean$kmeans_cluster <- clustered_data$kmeans_cluster -->

<!-- data_analysis_clean %>%  -->
<!--   mutate(kmeans_cluster = as.character(kmeans_cluster)) %>% -->
<!--   select(kmeans_cluster,delta_diff,beta_diff,dictator_diff,dictator_schoolmate_diff,risk_diff,publicgood_diff, -->
<!--                            trust_diff,trust_return_diff,comp, -->
<!--                           pca_socpref_interact,pca_timepref,pca_socpref_nointeract,pca_riskpref, -->
<!--                           math,read) %>%  -->
<!-- vtable::sumtable(group=c("kmeans_cluster"), group.long = TRUE, add.median = TRUE, simple.kable = TRUE, -->
<!--                  col.breaks = 7, -->
<!--                  title = "Summary Statistics for clusters generated by k-means") -->
<!-- ``` -->


```{r warning=FALSE,dpi=300, fig.width=14, fig.height=4}

# clustered hozzarendelese az osszes valtozohoz


table_one_tech <-data_analysis_clean_describe %>% 
  mutate(kmeans_cluster = as.character(kmeans_cluster)) %>%
  select(kmeans_cluster,delta,beta,dictator,dictator_schoolmate,risk,publicgood,
                           trust,trust_return,comp,
                          pca_socpref_interact,pca_timepref,pca_socpref_nointeract,pca_riskpref,
                          math,read)
table_one <-  arsenal::tableby(kmeans_cluster ~ ., stat= c("mean","median","range"), data = table_one_tech) 

kable(summary(table_one))

rm(table_one, table_one_tech)
```


```{r}
data_analysis_clean_describe %>% 
  ggplot(aes(x=delta, y = pca_socpref_interact))+ # col = as.factor(kmeans_cluster)))+
  geom_point()+
  theme_minimal()
```




```{r, include = FALSE}

table(data_analysis_clean_describe$kmeans_cluster, data_analysis_clean_describe$schoolid)

```


```{r, include = FALSE}

table(data_analysis_clean_describe$kmeans_cluster, data_analysis_clean_describe$classid)

```



```{r}
summary(lm(kmeans_cluster~as.factor(schoolid), data = data_analysis_clean_describe))
```


```{r, include = FALSE}
summary(lm(kmeans_cluster~as.factor(schoolid)+as.factor(classid), data = data_analysis_clean_describe))
```










<!-- -------- -->


<!-- # Clustering -->

<!-- Hierarchical clustering: -->


<!-- ```{r warning=FALSE,dpi=300, fig.width=10, fig.height=4} -->

<!-- hc.complete <- hclust(dist(data_analysis_clean_clusteringdata), method = "complete") -->
<!-- dendagram <- as.dendrogram(hc.complete) -->

<!-- LAB = rep("", nobs(dendagram)) -->
<!-- dendagram = dendextend::set(dendagram, "labels", LAB) -->

<!-- plot(dendextend::color_branches(dendagram, k = 6), main="Clusters created at height = 6.5", sub ="5 clusters with one outlier", leaflab = "none", horiz = F) -->
<!-- abline(h = 6.5, col = 'red') -->





<!-- # save the clusters: -->




<!-- ``` -->


<!-- K-means clustering: -->

<!-- As a first step, I check the optimal number of clusters using the elbow-plot; the results show that 5 is optimal, which is in line with the hierarchical clustering method. -->

<!-- ```{r warning=FALSE,dpi=300, fig.width=10, fig.height=4} -->
<!-- # elbow method: -->
<!-- set.seed(123) -->

<!-- # function to compute total within-cluster sum of square -->
<!-- wss <- function(k) { -->
<!--   kmeans(data_analysis_clean_clusteringdata, k, nstart = 100 )$tot.withinss -->
<!-- } -->

<!-- # Compute and plot wss for k = 1 to k = 15 -->
<!-- k.values <- 1:50 -->

<!-- # extract wss for 2-15 clusters -->
<!-- wss_values <- map_dbl(k.values, wss) -->

<!-- plot(k.values, wss_values, -->
<!--        type="b", pch = 19, frame = FALSE, -->
<!--        xlab="Number of clusters K", -->
<!--        ylab="Total within-clusters sum of squares") -->
<!-- ``` -->

<!-- Now knowing that the number of factors is the same, I run the K-means clustering algorithm for 5 clusters: -->


<!-- ```{r warning=FALSE,dpi=300, fig.width=10, fig.height=4} -->
<!-- set.seed(123469) -->
<!-- k5 <- kmeans(data_analysis_clean_clusteringdata, centers = 5, nstart = 10000) -->


<!-- fviz_cluster(k5, geom = "point",  data = data_analysis_clean_clusteringdata) + ggtitle("Clusters with 5 centers and 10,000 simulations")+ -->
<!--   theme_minimal() -->


<!-- # get classes: -->


<!-- # hierarchical clustering: -->
<!-- cut_complete <- cutree(hc.complete, h=6.5) -->

<!-- data_analysis_clean_clusteringdata$hierarchical_cluster <- cut_complete -->


<!-- #k-means: -->
<!-- data_analysis_clean_clusteringdata$kmeans_cluster <- k5$cluster -->
<!-- ``` -->



<!-- #### Cross-table of various observations using the two clustering algorithms: -->

<!-- ```{r} -->
<!-- table(data_analysis_clean_clusteringdata$hierarchical_cluster,data_analysis_clean_clusteringdata$kmeans_cluster) -->
<!-- ``` -->










